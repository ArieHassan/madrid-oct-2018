{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will find in this notebook some scrapy exercises to practise your scraping skills**.<br>**Remember:**\n",
    "- **To get each request status code to ensure you get the proper response from the web***\n",
    "- **To print the response text in each request to evaluate the what kind of info you are getting and its format.** \n",
    "- **To check for patterns in the response text to extract the data/info requested in each question.**\n",
    "- **To visit each url and take a look on its code through Chrome developer tool.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the libraries and modules you will need are included below. Feel free to explore other libraries i.e. scrapy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "# import scrapy\n",
    "from lxml import html\n",
    "from lxml.html import fromstring\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import random\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Download and display the content of robot.txt for Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check [here](http://www.robotstxt.org/robotstxt.html) to know more about ***robot.txt***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = \"https://en.wikipedia.org/robots.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xef\\xbb\\xbf# robots.txt for http://www.wikipedia.org/ and '\n"
     ]
    }
   ],
   "source": [
    "html = requests.get(url).content\n",
    "print(html[0:50]) # Imprimo solo una muestra para que no te desplegue tanta inf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Display the name of the most recently added dataset on data.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_2 ='http://catalog.data.gov/dataset?q=&sort=metadata_created+desc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Frigate Shoals Site P1A 11/1/2002 17-18M\n"
     ]
    }
   ],
   "source": [
    "html_2 = requests.get(url_2).content\n",
    "soup = BeautifulSoup(html_2, \"lxml\")\n",
    "recent_data = soup.select(\".dataset-content .dataset-heading\")\n",
    "print(recent_data[0].text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Number of datasets currently listed on data.gov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'http://www.data.gov/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300,295 datasets\n"
     ]
    }
   ],
   "source": [
    "html_3 = requests.get(url3).content\n",
    "soup = BeautifulSoup(html_3,\"lxml\")\n",
    "numb_ds = soup.select(\".header.banner.frontpage-search .container .text-center.getstarted a[href]\")\n",
    "print(numb_ds[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.wikipedia.com//wiki/File:Walt_Disney_1946.JPG',\n",
       " 'www.wikipedia.com//wiki/File:Walt_Disney_1942_signature.svg',\n",
       " 'www.wikipedia.com//wiki/File:Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'www.wikipedia.com//wiki/File:Trolley_Troubles_poster.jpg',\n",
       " 'www.wikipedia.com//wiki/File:Steamboat-willie.jpg',\n",
       " 'www.wikipedia.com//wiki/File:Walt_Disney_1935.jpg',\n",
       " 'www.wikipedia.com//wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg',\n",
       " 'www.wikipedia.com//wiki/File:Disney_drawing_goofy.jpg',\n",
       " 'www.wikipedia.com//wiki/File:DisneySchiphol1951.jpg',\n",
       " 'www.wikipedia.com//wiki/File:WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'www.wikipedia.com//wiki/File:Walt_disney_portrait_right.jpg',\n",
       " 'www.wikipedia.com//wiki/File:Walt_Disney_Grave.JPG',\n",
       " 'www.wikipedia.com//wiki/File:Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'www.wikipedia.com//wiki/File:Disney_Display_Case.JPG',\n",
       " 'www.wikipedia.com//wiki/File:Disney1968.jpg',\n",
       " 'www.wikipedia.com//wiki/File:P_vip.svg',\n",
       " 'www.wikipedia.com//wiki/File:Video-x-generic.svg',\n",
       " 'www.wikipedia.com//wiki/File:Flag_of_Los_Angeles_County,_California.svg',\n",
       " 'www.wikipedia.com//wiki/File:USA_flag_on_television.svg']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url4 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "html_4 = requests.get(url4).content\n",
    "soup = BeautifulSoup(html_4,\"lxml\")\n",
    "images = soup.select(\".image\")\n",
    "images_links = [\"www.wikipedia.com/\" + link.attrs[\"href\"] for link in images]\n",
    "display(images_links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_5 ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_5 = requests.get(url_5).content\n",
    "soup = BeautifulSoup(html_5,\"lxml\")\n",
    "links = soup.select(\"a[href^=/wiki]\") # Filtro por los links que me llevan a otras paginas. \n",
    "links_wiki = [\"www.wikipedia.com\" + i.attrs[\"href\"] for i in links] # aqui estan guardados los links \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_6 = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_6 = requests.get(url_6).content\n",
    "soup = BeautifulSoup(html_6,\"lxml\")\n",
    "changed_tit = soup.select(\".uscitem .usctitlechanged\")\n",
    "len(changed_tit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_7 = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BHADRESHKUMAR CHETANBHAI PATEL', 'LAMONT STEPHENSON', 'JASON DEREK BROWN', 'GREG ALYN CARLSON', 'SANTIAGO VILLALBA MEDEROS', 'RAFAEL CARO-QUINTERO', 'ROBERT WILLIAM FISHER', 'ALEXIS FLORES', 'ALEJANDRO ROSALES CASTILLO', 'YASER ABDEL SAID']\n"
     ]
    }
   ],
   "source": [
    "html_7 = requests.get(url_7).content\n",
    "soup = BeautifulSoup(html_7,\"lxml\")\n",
    "mw = soup.select(\".title\")\n",
    "top_10 = [e.text.strip() for e in mw]\n",
    "print(top_10[1:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_8 = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>20:29:42.8</td>\n",
       "      <td>36.68 N</td>\n",
       "      <td>121.31 W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>20:23:05.4</td>\n",
       "      <td>36.57 N</td>\n",
       "      <td>28.41 E</td>\n",
       "      <td>DODECANESE IS.-TURKEY BORDER REG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>20:15:03.4</td>\n",
       "      <td>37.73 N</td>\n",
       "      <td>20.76 E</td>\n",
       "      <td>IONIAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>20:00:19.9</td>\n",
       "      <td>32.61 N</td>\n",
       "      <td>48.57 E</td>\n",
       "      <td>WESTERN IRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>19:47:30.0</td>\n",
       "      <td>8.72 N</td>\n",
       "      <td>82.46 W</td>\n",
       "      <td>PANAMA-COSTA RICA BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>19:35:42.0</td>\n",
       "      <td>37.64 N</td>\n",
       "      <td>20.77 E</td>\n",
       "      <td>IONIAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>19:04:59.5</td>\n",
       "      <td>40.13 N</td>\n",
       "      <td>31.61 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>19:03:10.2</td>\n",
       "      <td>43.64 N</td>\n",
       "      <td>147.46 E</td>\n",
       "      <td>KURIL ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>19:02:48.0</td>\n",
       "      <td>35.89 N</td>\n",
       "      <td>27.47 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>18:22:55.0</td>\n",
       "      <td>9.65 S</td>\n",
       "      <td>117.25 E</td>\n",
       "      <td>SUMBAWA REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>18:22:29.0</td>\n",
       "      <td>15.44 N</td>\n",
       "      <td>96.28 W</td>\n",
       "      <td>OFFSHORE OAXACA, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>18:22:24.9</td>\n",
       "      <td>37.76 N</td>\n",
       "      <td>15.04 E</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>18:20:02.1</td>\n",
       "      <td>37.47 N</td>\n",
       "      <td>20.41 E</td>\n",
       "      <td>IONIAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>18:03:25.8</td>\n",
       "      <td>33.87 N</td>\n",
       "      <td>116.89 W</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>17:51:49.4</td>\n",
       "      <td>19.92 N</td>\n",
       "      <td>156.03 W</td>\n",
       "      <td>HAWAII REGION, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>17:49:06.1</td>\n",
       "      <td>37.31 N</td>\n",
       "      <td>20.68 E</td>\n",
       "      <td>IONIAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>17:47:03.0</td>\n",
       "      <td>12.34 N</td>\n",
       "      <td>87.15 W</td>\n",
       "      <td>NEAR COAST OF NICARAGUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>17:39:10.1</td>\n",
       "      <td>42.78 N</td>\n",
       "      <td>12.71 E</td>\n",
       "      <td>CENTRAL ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>17:12:55.2</td>\n",
       "      <td>33.48 N</td>\n",
       "      <td>116.79 W</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>16:47:08.4</td>\n",
       "      <td>19.34 N</td>\n",
       "      <td>154.99 W</td>\n",
       "      <td>HAWAII REGION, HAWAII</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date        time latitude longitude  \\\n",
       "1   2018-10-31  20:29:42.8  36.68 N  121.31 W   \n",
       "2   2018-10-31  20:23:05.4  36.57 N   28.41 E   \n",
       "3   2018-10-31  20:15:03.4  37.73 N   20.76 E   \n",
       "4   2018-10-31  20:00:19.9  32.61 N   48.57 E   \n",
       "5   2018-10-31  19:47:30.0   8.72 N   82.46 W   \n",
       "6   2018-10-31  19:35:42.0  37.64 N   20.77 E   \n",
       "7   2018-10-31  19:04:59.5  40.13 N   31.61 E   \n",
       "8   2018-10-31  19:03:10.2  43.64 N  147.46 E   \n",
       "9   2018-10-31  19:02:48.0  35.89 N   27.47 E   \n",
       "10  2018-10-31  18:22:55.0   9.65 S  117.25 E   \n",
       "11  2018-10-31  18:22:29.0  15.44 N   96.28 W   \n",
       "12  2018-10-31  18:22:24.9  37.76 N   15.04 E   \n",
       "13  2018-10-31  18:20:02.1  37.47 N   20.41 E   \n",
       "14  2018-10-31  18:03:25.8  33.87 N  116.89 W   \n",
       "15  2018-10-31  17:51:49.4  19.92 N  156.03 W   \n",
       "16  2018-10-31  17:49:06.1  37.31 N   20.68 E   \n",
       "17  2018-10-31  17:47:03.0  12.34 N   87.15 W   \n",
       "18  2018-10-31  17:39:10.1  42.78 N   12.71 E   \n",
       "19  2018-10-31  17:12:55.2  33.48 N  116.79 W   \n",
       "20  2018-10-31  16:47:08.4  19.34 N  154.99 W   \n",
       "\n",
       "                         region name  \n",
       "1                 CENTRAL CALIFORNIA  \n",
       "2   DODECANESE IS.-TURKEY BORDER REG  \n",
       "3                         IONIAN SEA  \n",
       "4                       WESTERN IRAN  \n",
       "5    PANAMA-COSTA RICA BORDER REGION  \n",
       "6                         IONIAN SEA  \n",
       "7                     WESTERN TURKEY  \n",
       "8                      KURIL ISLANDS  \n",
       "9         DODECANESE ISLANDS, GREECE  \n",
       "10         SUMBAWA REGION, INDONESIA  \n",
       "11           OFFSHORE OAXACA, MEXICO  \n",
       "12                     SICILY, ITALY  \n",
       "13                        IONIAN SEA  \n",
       "14               SOUTHERN CALIFORNIA  \n",
       "15             HAWAII REGION, HAWAII  \n",
       "16                        IONIAN SEA  \n",
       "17           NEAR COAST OF NICARAGUA  \n",
       "18                     CENTRAL ITALY  \n",
       "19               SOUTHERN CALIFORNIA  \n",
       "20             HAWAII REGION, HAWAII  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "html_8 = requests.get(url_8).content\n",
    "soup = BeautifulSoup(html_8,\"lxml\")\n",
    "latitud_s = soup.select(\"#tbody .tabev1\")\n",
    "latitud = [lat.text.replace(\"\\xa0\",\"\") for lat in latitud_s]\n",
    "cardinal = soup.select(\"#tbody .tabev2\")\n",
    "cardinal_2 = [lat.text.replace(\"\\xa0\",\"\") for lat in cardinal]\n",
    "cardinal_2 = [e for e in cardinal_2 if e.isalpha()]\n",
    "region = soup.select(\".tb_region\")\n",
    "region_s = [reg.text.replace(\"\\xa0\",\"\") for reg in region]\n",
    "time_date = soup.select(\"#tbody .tabev6 [href]\")\n",
    "date = [card.text.split()[0] for card in time_date]\n",
    "time = [card.text.split()[1] for card in time_date]\n",
    "latitude = latitud[0::2]\n",
    "longitude = latitud[1::2]\n",
    "lat_card = cardinal_2[0::2]\n",
    "long_card = cardinal_2[1::2]\n",
    "dictionary = {\"date\":date,\"time\":time,\"latitude2\":latitude,\"lat_card\":lat_card,\"long_card\":long_card,\"longitude2\":longitude,\"region name\":region_s}\n",
    "df = pd.DataFrame(dictionary)\n",
    "df[\"latitude\"] = df[\"latitude2\"] + \" \" + df[\"lat_card\"]\n",
    "df[\"longitude\"] = df[\"longitude2\"] + \" \" + df[\"long_card\"]\n",
    "df2 = df[[\"date\",\"time\",\"latitude\",\"longitude\",'region name']]\n",
    "df2.index +=1\n",
    "df2.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display the date, days, title, city, country of next 25 Hackevents as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_9 ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>days</th>\n",
       "      <th>title</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Nov-1</td>\n",
       "      <td>Thu-Fri</td>\n",
       "      <td>Rocket APT Challenge</td>\n",
       "      <td>Boston</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Nov-2</td>\n",
       "      <td>Fri-Sun</td>\n",
       "      <td>Hack Access Dublin 2017 - register your interest!</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>Ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nov-2</td>\n",
       "      <td>Fri-Sun</td>\n",
       "      <td>Disrupt Puerto Rico - Conference &amp; Hackathon</td>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Women's Hackathon</td>\n",
       "      <td>St. Louis</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>HackTheMidlands 3.0</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Women's Hackathon</td>\n",
       "      <td>St. Louis</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Nov-3</td>\n",
       "      <td>Sat-Sun</td>\n",
       "      <td>jacobsHack! 2018</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date     days                                              title  \\\n",
       "11  Nov-1  Thu-Fri                               Rocket APT Challenge   \n",
       "12  Nov-2  Fri-Sun  Hack Access Dublin 2017 - register your interest!   \n",
       "13  Nov-2  Fri-Sun       Disrupt Puerto Rico - Conference & Hackathon   \n",
       "14  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "15  Nov-3      Sat                                  Women's Hackathon   \n",
       "16  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "17  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "18  Nov-3  Sat-Sun                                HackTheMidlands 3.0   \n",
       "19  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "20  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "21  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "22  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "23  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "24  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "25  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "26  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "27  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "28  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "29  Nov-3      Sat                                  Women's Hackathon   \n",
       "30  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "31  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "32  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "33  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "34  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "35  Nov-3  Sat-Sun                                   jacobsHack! 2018   \n",
       "\n",
       "          city         country  \n",
       "11      Boston   United States  \n",
       "12      Dublin         Ireland  \n",
       "13    San Juan     Puerto Rico  \n",
       "14      Bremen         Germany  \n",
       "15   St. Louis             USA  \n",
       "16      Bremen         Germany  \n",
       "17      Bremen         Germany  \n",
       "18  Birmingham  United Kingdom  \n",
       "19      Bremen         Germany  \n",
       "20      Bremen         Germany  \n",
       "21      Bremen         Germany  \n",
       "22      Bremen         Germany  \n",
       "23      Bremen         Germany  \n",
       "24      Bremen         Germany  \n",
       "25      Bremen         Germany  \n",
       "26      Bremen         Germany  \n",
       "27      Bremen         Germany  \n",
       "28      Bremen         Germany  \n",
       "29   St. Louis             USA  \n",
       "30      Bremen         Germany  \n",
       "31      Bremen         Germany  \n",
       "32      Bremen         Germany  \n",
       "33      Bremen         Germany  \n",
       "34      Bremen         Germany  \n",
       "35      Bremen         Germany  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_9 = requests.get(url_9).content\n",
    "soup_9 = BeautifulSoup(html_9,\"lxml\")\n",
    "date_m = soup_9.select(\".date-month\")\n",
    "month = [month.text.split() for month in date_m]\n",
    "date_day = soup_9.select(\".date-day-number\")\n",
    "day = [day_.text.split() for day_ in date_day]\n",
    "date_days = soup_9.select(\".date-week-days\")\n",
    "week_days_ = [wday.text.split() for wday in date_days]\n",
    "week_days = [dia[0] for dia in week_days_]\n",
    "title_ = soup_9.select(\".title\")\n",
    "title = [tit.text for tit in title_]\n",
    "city_ = soup_9.select(\".city\")\n",
    "city = [city2.text.strip() for city2 in city_]\n",
    "country_ = soup_9.select(\".country\")\n",
    "country = [ctr.text.strip() for ctr in country_]\n",
    "date = [(a[0][0]+\"-\"+a[1][0]) for a in list(zip(month,day))]\n",
    "keys = [\"date\",\"days\", \"title\", \"city\", \"country\"]\n",
    "values = [date,week_days,title,city,country]\n",
    "dictionary = dict(zip(keys, values))\n",
    "df = pd.DataFrame(dictionary)\n",
    "df.index += 11\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_12 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('English', '5734000'),\n",
       " ('Español', '1481000'),\n",
       " ('日本語', '1124000'),\n",
       " ('Deutsch', '2228000'),\n",
       " ('Русский', '1502000'),\n",
       " ('Français', '2047000'),\n",
       " ('Italiano', '1467000'),\n",
       " ('中文', '1026000'),\n",
       " ('Português', '1007000'),\n",
       " ('Polski', '1303000')]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_12 =requests.get(url_12).content\n",
    "soup_12 = BeautifulSoup(html_12,\"lxml\")\n",
    "languages = soup_12.select(\"strong\")\n",
    "idiomas = [idiomas.text for idiomas in languages[1:]]\n",
    "number = soup_12.select(\"bdi\")\n",
    "numeros = [(str(num).split()[1][-1] + str(num).split()[2]+ str(num).split()[3].replace(\"+</bdi>\",\"\")) for num in number[:10]]\n",
    "lista = list(zip(idiomas,numeros))\n",
    "lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_13 = 'https://data.gov.uk/'\n",
    "html_13 = requests.get(url_13).content\n",
    "soup_13 = BeautifulSoup(html_13,\"lxml\")\n",
    "datasets  = soup_13.select(\"h2 [href]\")\n",
    "data_sets = [data.text for data in datasets]\n",
    "display(data_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. The total number of publications produced by the GAO (U.S. Government Accountability Office)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'54,912'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_14 = 'http://www.gao.gov/browse/date/custom'\n",
    "html_14 = requests.get(url_14).content\n",
    "soup_14 = BeautifulSoup(html_14,\"lxml\")\n",
    "number_of_pub = soup_14.select(\"h2.scannableTitle\")\n",
    "display(str(number_of_pub).split()[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_15 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "html_15 = requests.get(url_15).content\n",
    "soup_15 = BeautifulSoup(html_15,\"lxml\")\n",
    "languages = soup_15.select(\"a[href^=/wiki]\")\n",
    "lang = [idioma.text for idioma in languages[12:22]]\n",
    "number = soup_15.select(\"td\")\n",
    "numb = [numero.text for numero in number[2::4]][:10]\n",
    "zipped = list(zip(lang,numb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native Speakers in Millions 2007 (2010)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin</td>\n",
       "      <td>935 (955)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>390 (405)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>365 (360)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>295 (310)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>280 (295)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>205 (215)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>200 (205)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Russian</td>\n",
       "      <td>160 (155)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125 (125)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Punjabi</td>\n",
       "      <td>95 (100)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Language Native Speakers in Millions 2007 (2010)\n",
       "0    Mandarin                               935 (955)\n",
       "1     Spanish                               390 (405)\n",
       "2     English                               365 (360)\n",
       "3       Hindi                               295 (310)\n",
       "4      Arabic                               280 (295)\n",
       "5  Portuguese                               205 (215)\n",
       "6     Bengali                               200 (205)\n",
       "7     Russian                               160 (155)\n",
       "8    Japanese                               125 (125)\n",
       "9     Punjabi                                95 (100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top10 = pd.DataFrame(zipped)\n",
    "top10.columns = [\"Language\",\"Native Speakers in Millions 2007 (2010)\"]\n",
    "display (top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city:\n"
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
